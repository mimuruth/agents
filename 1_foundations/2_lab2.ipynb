{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads\n",
    "`ollama serve`\tStarts Ollama on your local system.\n",
    "`ollama create <new_model>`\tCreates a new model from an existing one for customization or training.\n",
    "`ollama show <model>`\tDisplays details about a specific model, such as its configuration and release date.\n",
    "`ollama run <model>`\tRuns the specified model, making it ready for interaction.\n",
    "`ollama pull <model>`\tDownloads the specified model to your system.\n",
    "`ollama list`\tLists all the downloaded models. The same as ollama ls\n",
    "`ollama ps`\tShows the currently running models.\n",
    "`ollama stop <model>`\tStops the specified running model.\n",
    "`ollama rm <model>`\tRemoves the specified model from your system.\n",
    "`ollama help`\tProvides help about any command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <ul style=\"color:#ff7800; padding-left: 20px; margin-top: 0;\">\n",
    "                <li>Each model judges all responses (except its own bias isn't excluded — you could filter that later).</li>\n",
    "                <li>You gather all ranking lists, store them in a NumPy matrix.</li>\n",
    "                <li>Then compute average rankings across all judges.</li>\n",
    "                <li>Result: a more robust, consensus-based evaluation.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each competitor now becomes a judge\n",
    "\n",
    "ensemble_rankings = []\n",
    "\n",
    "# Prepare the judging prompt\n",
    "for i, model_name in enumerate(competitors):\n",
    "    print(f\"\\nGetting rankings from {model_name}...\")\n",
    "\n",
    "    judge_prompt = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"1\", \"2\", \"3\", ...]}}\n",
    "\n",
    "Do not use labels like \"competitor 1\" or \"model 2\". Just use plain strings like \"1\", \"2\", etc.\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with only the JSON. Do not include any explanation, markdown, or code blocks.\n",
    "\"\"\"\n",
    "\n",
    "    judge_messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "\n",
    "    try:\n",
    "        if model_name.startswith(\"claude\"):\n",
    "            claude = Anthropic()\n",
    "            response = claude.messages.create(model=model_name, messages=judge_messages, max_tokens=1000)\n",
    "            content = response.content[0].text\n",
    "\n",
    "        elif model_name.startswith(\"gemini\"):\n",
    "            gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "            response = gemini.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "            content = response.choices[0].message.content\n",
    "\n",
    "        elif model_name.startswith(\"deepseek\"):\n",
    "            deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "            response = deepseek.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "            content = response.choices[0].message.content\n",
    "\n",
    "        elif model_name.startswith(\"llama-3.3\"):\n",
    "            groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "            response = groq.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "            content = response.choices[0].message.content\n",
    "\n",
    "        elif model_name.startswith(\"llama3.2\"):\n",
    "            ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "            response = ollama.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "            content = response.choices[0].message.content\n",
    "\n",
    "        else:  # Default to OpenAI\n",
    "            openai = OpenAI()\n",
    "            response = openai.chat.completions.create(model=model_name, messages=judge_messages)\n",
    "            content = response.choices[0].message.content\n",
    "\n",
    "        # Try to extract JSON from content\n",
    "        content = content.strip()\n",
    "        json_match = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n",
    "        if not json_match:\n",
    "            raise ValueError(\"No valid JSON block found in response.\")\n",
    "        \n",
    "        json_str = json_match.group(0)\n",
    "        result = json.loads(json_str)\n",
    "\n",
    "        # Normalize format like \"competitor 3\" to \"3\"\n",
    "        cleaned_result = []\n",
    "        for r in result[\"results\"]:\n",
    "            digits = re.search(r\"\\d+\", r)\n",
    "            if digits:\n",
    "                cleaned_result.append(digits.group())\n",
    "            else:\n",
    "                raise ValueError(f\"Could not extract number from ranking item: {r}\")\n",
    "\n",
    "        ensemble_rankings.append(cleaned_result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error judging with {model_name}: {e}\")\n",
    "        print(\"Raw response:\\n\", content)\n",
    "\n",
    "\n",
    "# Tally average ranks\n",
    "rank_matrix = np.zeros((len(ensemble_rankings), len(competitors)))\n",
    "\n",
    "for judge_idx, ranking in enumerate(ensemble_rankings):\n",
    "    for rank_pos, competitor_number in enumerate(ranking):\n",
    "        comp_index = int(competitor_number) - 1\n",
    "        rank_matrix[judge_idx][comp_index] = rank_pos + 1  # ranks are 1-based\n",
    "\n",
    "avg_ranks = rank_matrix.mean(axis=0)\n",
    "sorted_indices = np.argsort(avg_ranks)\n",
    "\n",
    "print(\"\\n=== Ensemble Average Ranking ===\\n\")\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    print(f\"Rank {i+1}: {competitors[idx]} (Average rank: {avg_ranks[idx]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install matplotlib\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Ensemble Average Ranking ===\\n\")\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    print(f\"Rank {i+1}: {competitors[idx]} (Average rank: {avg_ranks[idx]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Ensemble Average Ranks ---\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Sort for visualization\n",
    "# sorted_avg_ranks = avg_ranks[sorted_indices]\n",
    "# sorted_labels = [competitors[i] for i in sorted_indices]\n",
    "\n",
    "# # Create the bar chart\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# bars = plt.barh(sorted_labels, sorted_avg_ranks, color='skyblue', edgecolor='black')\n",
    "\n",
    "# # Highlight the top performer\n",
    "# bars[0].set_color('#ff7800')\n",
    "\n",
    "# plt.xlabel(\"Average Rank (Lower is Better)\", fontsize=12)\n",
    "# plt.title(\"LLM Competitor Average Ranking by Peer Judgement\", fontsize=14)\n",
    "# plt.gca().invert_yaxis()  # Top rank first\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Annotate bars with values\n",
    "# for bar in bars:\n",
    "#     width = bar.get_width()\n",
    "#     plt.text(width + 0.1, bar.get_y() + bar.get_height()/2, f\"{width:.2f}\", va='center')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <h3 style=\"color:#ff7800;\">Ask the LLM to analyze what Agentic behaviors each competitor demonstrated in their response.</h3>\n",
    "            <ul style=\"padding-left: 20px; color:#ff7800;\">\n",
    "                <li>Adds a new agentic design pattern: self-reflection and critique</li>\n",
    "                <li>Uses the same question and responses (<code>question</code>, <code>together</code>)</li>\n",
    "                <li>Adds a new round of analysis from the OpenAI model</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agentic Design Pattern: Self-Reflective Critic Agent ---\n",
    "\n",
    "reflection_prompt = f\"\"\"\n",
    "You are a critical analyst trained to evaluate whether a model response demonstrates agentic behavior.\n",
    "\n",
    "Each of the following LLM responses was to this prompt:\n",
    "{question}\n",
    "\n",
    "Agentic responses are characterized by initiative, reasoning toward a goal, planning steps, self-monitoring, or making assumptions and refining them. For each response, analyze:\n",
    "\n",
    "1. Did the model demonstrate any agentic behavior?\n",
    "2. If yes, what type? (e.g., goal-directed reasoning, planning, self-reflection)\n",
    "3. How well did the behavior contribute to the response's usefulness?\n",
    "\n",
    "Provide a short justification per response and label it with the original competitor number.\n",
    "\n",
    "Respond in JSON like this:\n",
    "{{\n",
    "  \"analysis\": [\n",
    "    {{\n",
    "      \"competitor\": 1,\n",
    "      \"agentic_behavior\": true,\n",
    "      \"type\": [\"goal-directed reasoning\"],\n",
    "      \"justification\": \"...\"\n",
    "    }},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the new reflection agent\n",
    "reflection_messages = [{\"role\": \"user\", \"content\": reflection_prompt}]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=reflection_messages,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "reflection_result = response.choices[0].message.content\n",
    "print(reflection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Parse and format results\n",
    "try:\n",
    "    reflection_data = json.loads(reflection_result)\n",
    "    print(\"\\nAgentic Behavior Analysis:\\n\")\n",
    "    for entry in reflection_data[\"analysis\"]:\n",
    "        print(f\"Competitor {entry['competitor']}:\")\n",
    "        print(f\"  Agentic Behavior: {entry['agentic_behavior']}\")\n",
    "        if entry['agentic_behavior']:\n",
    "            print(f\"  Type: {', '.join(entry['type'])}\")\n",
    "        print(f\"  Justification: {entry['justification']}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"Could not parse reflection result:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
